<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>python多进程/多线程模块总览</title>
      <link href="/2019/05/24/pt_overview/"/>
      <url>/2019/05/24/pt_overview/</url>
      
        <content type="html"><![CDATA[<p>占个坑，这篇文章用来阐述开启这个项目的动机，理一下这个项目的思路。顺便画一下python里面多线程/多进程模块依赖关系的图示</p>]]></content>
      
      
      <categories>
          
          <category> 操作系统 </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>让自己程序更好用的小技巧（一）使用配置文件替代手动输入参数</title>
      <link href="/2019/03/17/5183/"/>
      <url>/2019/03/17/5183/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在我们开发程序的过程中，我们一个程序的变量初始值可能是要随着每次运行而改变的，比如我今天写了一个程序计算1+2的结果，明天想计算3+4的结果，那么要手动修改程序里面的变量改成3和4再重新编译运行输出结果吗？这是不现实的，作为程序的开发者，也许可以轻易地做到这点，但是程序的用户的绝对不会愿意去这么干的。所以一般我们会在程序运行的过程中让用户手动输入相加的两个数值，这样子这个程序就可以用在任意两个整数相加了，就可以“编写一次，重复使用”了。</p><p>要让用户向程序中输入数据，通俗的说无非就那么几种做法：</p><ol><li>在运行程序的时候向main函数传参</li><li>在程序运行的过程中使用输入语句从终端读取字符（在没有图形界面的情况下）</li><li>通过图形界面进行数据输入操作（在拥有图形界面的情况下）</li><li>从文件中读取数据</li><li>通过网络通信得到数据</li></ol><p>图形界面一般是用户最喜爱的操作方式，然而在以下情况下，图形界面并不是一个好的选择：</p><ol><li>程序的运行环境没有图形界面，</li><li>编程语言本身极难开发图形界面应用程序</li><li>程序的核心功能不在于界面，没有编写图形界面的必要</li><li>编写图形界面的时间成本过高，无法接受如此庞大的时间支出</li><li>每次运行需要录入的数据过多，手动在图形界面中输入非常浪费时间</li></ol><p>如果符合以上其中一个要求，又希望能向程序中输入自定义的变量，且又希望程序的运行过程能更好的减少人工干预。那么做法1和做法4是最合理的。在linux中也确实是这么做的。linux使用者大多都是工作在“黑窗口”下，而为了程序启动时高效地迅速输入各变量的初始值，linux程序一般会提供向main函数传参和读取配置文件这两种方案。</p><p>这两种方案各有优劣吧，毕竟两者都能沿用至今，肯定是都有自己的好处。在这里我仅讨论其中一种情况。如果程序的自定义初始化参数过多，向main函数传参的启动方式就会变得十分难用，参数列表可能会很长。用户可能也难以记住每个参数的意义，下一次启动时也需要再重复输入参数，参数无法保存。所以这里主要讨论常见的几种配置文件的语法格式以及对我们平常coding的帮助。</p><p>常见的配置文件格式有：</p><ul><li>.conf</li><li>.xml</li><li>.json</li></ul><h1 id="conf格式"><a href="#conf格式" class="headerlink" title=".conf格式"></a>.conf格式</h1><p>严格来说这个可能不算一种格式，因为它不像xml或json那样有严格的语法定义。甚至有时候用这个后缀名就仅仅只是为了向用户说明这个是一个配置文件而已。所以就没有特别明确的编写规则了，主要还是由程序本身决定如何解析这个文件，程序开发者自己确定解析规则之后再跟用户约定书写方法。</p><p>在linux中以这个后缀作为配置文件的软件非常普遍，包括不少系统配置文件也是使用这个后缀名。对于<strong>系统中</strong>的conf文件，解析方式还是比较通用的，我们能从其中总结出大概的语法（只能是大概，具体配置方式一定要看该文件官方给出的书写方式）。即使是不以此后缀命名的系统配置文件，也大多遵从这个书写方法。</p><p>而有些<strong>第三方的软件</strong>则在系统conf文件约定俗成的书写方法的基础上增加了自己的解析规则，这就使conf文件看上去格式更加花样众多了。具体的配置方法也只能必须按照软件官方规定的书写方法进行配置。</p><p>下面先介绍所谓的“基本写法”吧，写法很简单：</p><ol><li>在行首以#号开头则视为注释，该行无效</li><li>对于有效的行，以一个或多个空格分隔，“关键字”写在前，参数写在后，若有多个参数则分别在后面写上，参数之间当然也要分隔。可以认为每一行就是个一对一或者一对多的键值对</li></ol><p>下面截图举例可能经常见到的一些conf文件：<br>（这里只关心书写方法，不深究配置项的具体功能，这不是本文讨论范围）</p><p><strong>/etc/resolv.conf</strong><br>这里头两行就是注释（甚至vim编辑器已经帮我们做了语法着色了）。下面4行是4个配置项，前面是关键字，而后空格分隔，紧跟着就是参数。<br><img src="http://120.24.1.93/blogimg/5183/751_1.png" alt=""></p><p><strong>/etc/hosts</strong><br>这个文件虽然无后缀名，但也遵循conf“基本写法”<br><img src="http://120.24.1.93/blogimg/5183/747_1.png" alt=""></p><p><strong>/etc/network/interfaces</strong><br>这是用来设置网卡接口ip的文件，也遵循conf的“基本写法”<br><img src="http://120.24.1.93/blogimg/5183/749_1.png" alt=""></p><p><strong>redis.conf</strong><br>redis是第三方软件，如果有需要用到的话。使用过程中会接触到这个配置文件，它的书写格式也很规规矩矩<br><img src="http://120.24.1.93/blogimg/5183/753_1.png" alt=""></p><p><strong>nginx.conf</strong><br>nginx的配置文件，这是个比较常用的软件。他的配置文件写法就比较奇怪，每一个配置项的末尾都以分号结束，而且还包含了嵌套结构。以至于vim甚至无法对其进行语法着色了。像这种配置文件就不是很通用，你只能按照nginx定好的规则来写，并且该规则也只适用于这款软件的配置。</p><p><img src="http://120.24.1.93/blogimg/5183/759_1.png" alt=""></p><p><strong>apache2.conf</strong><br>这个是apache服务的配置文件，同样是对“基本写法”进行了扩展的，他能将一组配置项用一个带尖括号的标签包围起来。<br><img src="http://120.24.1.93/blogimg/5183/761_1.png" alt=""></p><h1 id="xml格式"><a href="#xml格式" class="headerlink" title=".xml格式"></a>.xml格式</h1><p>相对于前面那个格式而言，xml格式就很规范了，关于这个知识甚至还能单独出一本书。这里只能大概讨论几个主要特征，盘点它和其他格式的不同之处。</p><p>首先贴出一份hadoop中core-site.xml的配置文件内容吧，大家不需要知道配置项的具体作用，我们主要看文件的格式。</p><pre><code class="xml">&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://127.0.0.1:9000&lt;/value&gt;        &lt;description&gt;127.0.0.1为服务器IP地址，其实也可以使用主机名&lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;io.file.buffer.size&lt;/name&gt;        &lt;value&gt;131072&lt;/value&gt;        &lt;description&gt;该属性值单位为KB，131072KB即为默认的64M&lt;/description&gt;    &lt;/property&gt;    &lt;!--- 这行是被注释的内容 --&gt;&lt;/configuration&gt;</code></pre><p>从这里可以看出，标签之间就是树形结构，如果学过html之类的标记语言，就会很熟悉了。各个配置项都单独放在一个property标签中，配置项的属性又分别放在name、value、description标签中。</p><p>如果换作.conf格式，则<strong>有可能</strong>是这个样子</p><pre><code class="conf">#127.0.0.1为服务器IP地址，其实也可以使用主机名fs.defaultFS hdfs://127.0.0.1:9000 #该属性值单位为KB，131072KB即为默认的64Mio.file.buffer.size 131072</code></pre><p>如果换作json格式，则<strong>有可能</strong>是这个样子</p><pre><code class="json">{    &quot;property&quot;: [        {            &quot;name&quot;: &quot;fs.defaultFS&quot;,            &quot;value&quot;: &quot;hdfs://127.0.0.1:9000&quot;,            &quot;description&quot;:  &quot;127.0.0.1为服务器IP地址，其实也可以使用主机名&quot;        },        {            &quot;name&quot;: &quot;io.file.buffer.size&quot;,            &quot;value&quot;: &quot;131072&quot;,            &quot;description&quot;: &quot;该属性值单位为KB，131072KB即为默认的64M&quot;        }    ]}</code></pre><h1 id="json格式"><a href="#json格式" class="headerlink" title=".json格式"></a>.json格式</h1><p>全称“javascript对象表示法”，学过js的人应该都能看得懂，其实就是一个键值对和数组经过嵌套形成的一个js对象，将其序列化成文本之后就是我们的json格式。需要注意的是，json文件不存在注释，除了对象的信息之外，多余的字符会导致文件解析失败。如果确实需要注释，那么可以添加一个用不上的字段用来保存注释信息。</p><p>比如下面这个就是某飞机客户端的配置文件，每次启动程序的时候程序就会读取这个文件，解析里面的配置值，装填到程序当中。如果没有这个配置文件，那我们每次启动就要手动输入这么多配置参数，会十分痛苦。第一次设置时，程序将参数保存在配置文件中，此后我们只需要鼠标双击打开客户端就可以愉快的使用了。</p><pre><code class="json">{    &quot;configs&quot;: [{        &quot;server&quot;: &quot;66.66.66.66&quot;,        &quot;server_port&quot;: 6666,        &quot;password&quot;: &quot;1234&quot;,        &quot;method&quot;: &quot;rc4-md5&quot;,        &quot;plugin&quot;: &quot;&quot;,        &quot;plugin_opts&quot;: &quot;&quot;,        &quot;plugin_args&quot;: &quot;&quot;,        &quot;remarks&quot;: &quot;&quot;,        &quot;timeout&quot;: 5    }],    &quot;strategy&quot;: null,    &quot;index&quot;: 0,    &quot;global&quot;: true,    &quot;enabled&quot;: false,    &quot;shareOverLan&quot;: false,    &quot;isDefault&quot;: false,    &quot;localPort&quot;: 1080,    &quot;portableMode&quot;: false,    &quot;pacUrl&quot;: null,    &quot;useOnlinePac&quot;: false,    &quot;secureLocalPac&quot;: true,    &quot;availabilityStatistics&quot;: false,    &quot;autoCheckUpdate&quot;: true,    &quot;checkPreRelease&quot;: false,    &quot;isVerboseLogging&quot;: false,    &quot;logViewer&quot;: {        &quot;topMost&quot;: false,        &quot;wrapText&quot;: false,        &quot;toolbarShown&quot;: false,        &quot;Font&quot;: &quot;Consolas, 8pt&quot;,        &quot;BackgroundColor&quot;: &quot;Black&quot;,        &quot;TextColor&quot;: &quot;White&quot;    },    &quot;proxy&quot;: {        &quot;useProxy&quot;: false,        &quot;proxyType&quot;: 0,        &quot;proxyServer&quot;: &quot;&quot;,        &quot;proxyPort&quot;: 0,        &quot;proxyTimeout&quot;: 3    },    &quot;hotkey&quot;: {        &quot;SwitchSystemProxy&quot;: &quot;&quot;,        &quot;SwitchSystemProxyMode&quot;: &quot;&quot;,        &quot;SwitchAllowLan&quot;: &quot;&quot;,        &quot;ShowLogs&quot;: &quot;&quot;,        &quot;ServerMoveUp&quot;: &quot;&quot;,        &quot;ServerMoveDown&quot;: &quot;&quot;,        &quot;RegHotkeysAtStartup&quot;: false    }}</code></pre><h1 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h1><p>某天下午，有同学叫我帮写一个QQ机器人<br><img src="http://120.24.1.93/blogimg/5183/763_1.png" alt=""></p><p>既然是转发机器人，那么必定会涉及转发规则，监听来自谁的消息，转发给谁和谁，这些都是需要用户自己制定的规则。如何让同学很方便地编写这个规则表就是一个问题了，虽然同学也是学计算机的，但是人家并不懂python代码，总不可能让人家每次改动规则都要去改变一次源代码吧？</p><p>这个时候，配置文件就派上用场了。（不要问我为什么不做图形界面，做图形界面消耗的时间比机器人的业务逻辑花费时间要多两倍以上，实在是提不起那个劲）</p><p>首先是一个数组，数组里面每一个元素是一个转发规则，每个转发规则监听一个消息来源，因为单个来源可以对应多个去路，所以去路是一个数组，每个元素保存一个号码。</p><pre><code class="json">{    [        {            &quot;来源&quot;: {                &quot;消息类型&quot;: &quot;group&quot;,                &quot;号码&quot;: 111111            },            &quot;去路&quot;: [                {                    &quot;消息类型&quot;: &quot;group&quot;,                    &quot;号码&quot;: 111111,                    &quot;显示来源&quot;: true,                    &quot;来源备注&quot;: &quot;复读来自测试群的消息：&quot;                },                {                    &quot;消息类型&quot;: &quot;private&quot;,                    &quot;号码&quot;: 222222,                    &quot;显示来源&quot;: true,                    &quot;来源备注&quot;: &quot;转发来自测试群的消息：&quot;                }            ]        },        {            &quot;来源&quot;: {                &quot;消息类型&quot;: &quot;private&quot;,                &quot;号码&quot;: 222222            },            &quot;去路&quot;: [                {                    &quot;消息类型&quot;: &quot;group&quot;,                    &quot;号码&quot;: 111111,                    &quot;显示来源&quot;: true,                    &quot;来源备注&quot;: &quot;转发来自大号的消息：\n&quot;                },                {                    &quot;消息类型&quot;: &quot;private&quot;,                    &quot;号码&quot;: 222222,                    &quot;显示来源&quot;: true,                    &quot;来源备注&quot;: &quot;复读来自大号的消息：\n&quot;                }            ]        }    ]}</code></pre><p>事实证明这个做法还是比较可行的，使用过程中没遇到糟心的问题。</p><p><img src="http://120.24.1.93/blogimg/5183/765_1.png" alt=""></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://www.cnblogs.com/yinghun/p/6230436.html" target="_blank" rel="noopener">https://www.cnblogs.com/yinghun/p/6230436.html</a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>Hive与Hbase整合，共享表和数据</title>
      <link href="/2019/02/12/5/"/>
      <url>/2019/02/12/5/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>首先参考这两篇文章</p><p><a href="https://yq.aliyun.com/articles/32468" target="_blank" rel="noopener">https://yq.aliyun.com/articles/32468</a></p><p><a href="https://www.cnblogs.com/dongdone/p/5681295.html" target="_blank" rel="noopener">https://www.cnblogs.com/dongdone/p/5681295.html</a></p><p>上面提到可以使用Hive提供的jar包实现这两个框架的整合</p><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><p>这里需要使用–auxpath 参数引入两个jar包：<br>hive-hbase-handler-x.x.x.jar和zookeeper-x.x.x.jar</p><p>每个教程里面写的jar包路径都是不一样的，所以我要自己动手的话，当然也是要找一下这些jar包的位置啦</p><p>我的Hive安装文件夹是 /usr/local/hive/</p><p>切到安装目录下面看看</p><pre><code class="sh">cd /usr/local/hive/ls lib | grep -E &quot;hbase-handler|zookeeper&quot;</code></pre><p>可以看到确实是有这两个jar文件的</p><p><img src="http://120.24.1.93/blogimg/5/569_1.png" alt=""></p><p>所以，在我的电脑上这两个文件的完整路径为</p><p>/usr/local/hive/lib/hive-hbase-handler-3.1.1.jar</p><p>/usr/local/hive/lib/zookeeper-3.4.6.jar</p><p>此外还需要-hiveconf参数输入两组配置数值：hbase.master和hbase.zookeeper.quorum</p><p>hbase.master这个参数，需要跟Hbase中hbase-site.xml里面的设置对应上，我这里是localhost:9000</p><p>hbase.zookeeper.quorum的值也是对应的，我这里是localhost</p><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>所以，我这边的完整的启动命令就是</p><pre><code class="sh">hive --auxpath /usr/local/hive/lib/hive-hbase-handler-3.1.1.jar,/usr/local/hive/lib/zookeeper-3.4.6.jar -hiveconf hbase.master=localhost:9000 -hiveconf hbase.zookeeper.quorum=localhost</code></pre><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>上面的命令若没有错误，是可以启动成功的</p><p>现在就可以在Hive中建立一个Hbase的关联表了，使用<br>CREATE EXTERNAL TABLE可以建立一个外部表，设置好映射关系就可以直接把Hbase上面的表映射过来了。</p><p>然而此时并没有Hbase的表，那就先开启另一个终端，进入hbase shell，先在里面创建一个表，随便定一个表名叫student22吧，就是我们学数据库最熟悉的学生表。只有一个列族：info。列族里面只有一个列：name。</p><pre><code class="sql">create &#39;student22&#39;,&#39;info&#39;put &#39;student22&#39;,&#39;1001&#39;,&#39;info:name&#39;,&#39;zhangsan&#39;scan &#39;student22&#39;</code></pre><p><img src="http://120.24.1.93/blogimg/5/573_1.png" alt=""></p><p>这样就有了一个表，表里面有了一条数据</p><p>接下来就回到Hive的命令行里面创建一个表了</p><pre><code class="sql">CREATE EXTERNAL TABLE student44(id string, name string)STORED BY &#39;org.apache.hadoop.hive.hbase.HBaseStorageHandler&#39; WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,info:name&quot;) TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;student22&quot;);</code></pre><p><img src="http://120.24.1.93/blogimg/5/575_1.png" alt=""></p><p>这时候，Hive中的student44这个表就跟Hbase中的student22这个表对应上了，其中id这个字段映射的是student22的行号，name这个字段映射的是info列族里面的name这个列。在Hive的命令行里面查询一条语句看看</p><pre><code class="sql">select * from student44</code></pre><p><img src="http://120.24.1.93/blogimg/5/577_1.png" alt=""></p><p>可以发现成功读取到了Hbase的数据。</p><p>再去Hbase里面添加一条新数据，看看从Hive能不能访问到。</p><p><img src="http://120.24.1.93/blogimg/5/579_1.png" alt=""></p><p><img src="http://120.24.1.93/blogimg/5/581_1.png" alt=""></p><p>完全没问题。</p><h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><p>上面已经完成了大部分人想要的效果，然而我自己的这个Hive是用python通过hiveserver2来操作的。那么我想尝试一下通过hiveserver2进行的操作是否也是同步的呢？</p><p>首先还是准备一段python代码，注意这里，数据库名称是default。因为之前我们创建student44这个表之前并没有指定use哪个数据库，所以这个表是被归到default这个数据库中的。</p><pre><code class="python">from pyhive import hiveconn = hive.Connection(host=&#39;127.0.0.1&#39;, port=10000,  database=&#39;default&#39; ,username=&#39;hadoop&#39;)cursor=conn.cursor()cursor.execute(&#39;select * from student44&#39;)for result in cursor.fetchall():     print(result)conn.close()</code></pre><p>然后启动hiveserver2，这里为了稳妥一些，没有使用快捷启动，而是使用了加参数的启动方式</p><pre><code class="sh">hive --auxpath /usr/local/hive/lib/hive-hbase-handler-3.1.1.jar,/usr/local/hive/lib/zookeeper-3.4.6.jar -hiveconf hbase.master=localhost:9000 -hiveconf hbase.zookeeper.quorum=localhost --service hiveserver2</code></pre><p>运行成功，没有问题。<br><img src="http://120.24.1.93/blogimg/5/600_1.png" alt=""></p><p>关掉已经打开的hive服务，尝试用快速启动</p><pre><code class="sh">hiveserver2</code></pre><p>再运行一遍python程序，结果也是可以出来的，截图就不重复了，跟上面这张截图一样。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hives </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>使用python3通过pyhive操作Hive</title>
      <link href="/2019/02/11/4/"/>
      <url>/2019/02/11/4/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><div style="display: none"><br>在某场比赛的作品中，需要用python操作Hive数据仓库，控制其执行一些查询语句并将其查询结果存入mysql中。也就是同时使用python驱动两款数据库，HiveSQL语句承担数据分析的功能，mysql则用来存储分析出来的结果。因为分析结果只是一些简单的报表类型数据，数据量级很小，用普通的关系型数据库存储是比较合理的，若是<br></div><p>某个项目需要python操作Hive数据仓库，对比了网上的三种方案（pyhs2、pyhive、impyla），其中pyhs2已经不再维护，剩下两款都不错，随便挑选了其中一款。就决定用pyhive实现了。</p><h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><p>本人使用的开发环境：</p><table><thead><tr><th>开发环境</th><th>版本</th><th>安装位置</th><th>备注</th></tr></thead><tbody><tr><td>操作系统</td><td>ubuntu 16.04</td><td></td><td></td></tr><tr><td>python</td><td>python 3.5</td><td>系统自带</td><td></td></tr><tr><td>Hive</td><td>3.1.1</td><td>/usr/local/hive/</td><td></td></tr><tr><td>hadoop</td><td>2.7.7</td><td>/usr/local/hadoop/</td><td>伪分布式</td></tr></tbody></table><h2 id="安装库和依赖"><a href="#安装库和依赖" class="headerlink" title="安装库和依赖"></a>安装库和依赖</h2><p>要使用pyhive，首先要安装一些依赖</p><pre><code class="sh">pip3 install saslpip3 install thriftpip3 install thrift-saslpip3 install PyHive</code></pre><p>安装sasl容易失败，我第一次安装的时候也是出来了奇奇怪怪的报错提示，按照别人博客上面提的解决方法，去<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#sasl" target="_blank" rel="noopener">某个网站</a> 寻找下载wheel包，但是发现上面提供的是win32/64的安装包，拷到虚拟机安装会提示平台不匹配</p><p>最终进到了<a href="https://pypi.org/project/sasl/0.1.3/#description" target="_blank" rel="noopener">sasl的官网</a>，上面提到了需要安装一些依赖</p><pre><code class="sh">apt-get install python-dev libsasl2-dev gcc</code></pre><p>装完了上面这些东西之后，再安装sasl就一次成功了</p><pre><code class="sh">pip3 install sasl --userpip3 install thrift-sasl --user</code></pre><p>需要安装的东西也就这点了，不需要编译安装任何麻烦的东西</p><h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><p>装完可以连接hive的库之后，首先要启动hadoop</p><pre><code class="sh">/usr/local/hadoop/sbin/start-all.sh</code></pre><p>hadoop跑起来之后再启动Hive的thrift服务</p><pre><code class="sh"> $HIVE_HOME/bin/hiveserver2</code></pre><p> 或者</p><pre><code class="sh"> $HIVE_HOME/bin/hive --service hiveserver2</code></pre><p> 上面两种方法都可以启动，然而我之前已经将将$HIVE_HOME/bin/加入PATH了，所以在我的虚拟机中是这么启动的</p><pre><code class="sh">hiveserver2</code></pre><p>启动了hiveserver2之后，我们可以通过10000端口去连接数据库进行操作，也可以通过访问端口10002在web页面查看Hive的一些基本信息。</p><p><img src="http://120.24.1.93/blogimg/4/559_1.png" alt="559_1.png"></p><p>能正常打开网页，说明服务启动是没有问题的。</p><p>这个hiveserver2，启动要很久，如果暂时打不开可以等一两分钟。</p><h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>在编程之前，先修改一下hive和hadoop的配置文件</p><pre><code class="sh">gedit /usr/local/hive/conf/hive-site.xml</code></pre><p>将其修改成</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;    &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;    &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;    &lt;value&gt;hive&lt;/value&gt;    &lt;description&gt;username to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;    &lt;value&gt;hive&lt;/value&gt;    &lt;description&gt;password to use against metastore database&lt;/description&gt;  &lt;/property&gt;  &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;    &lt;description&gt;&lt;/description&gt;   &lt;/property&gt;   &lt;property&gt;    &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;    &lt;value&gt;*&lt;/value&gt;    &lt;description&gt;&lt;/description&gt;   &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>hadoop的core-site.xml也改一下</p><pre><code class="sh">gedit /usr/local/hadoop/etc/hadoop/core-site.xml </code></pre><p>修改成</p><pre><code class="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;        &lt;value&gt;*&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;        &lt;value&gt;*&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;</code></pre><p>配置文件这个问题卡了我很久，感谢这两篇文章的作者</p><ol><li><a href="https://blog.csdn.net/GK_kk/article/details/68924943" target="_blank" rel="noopener">hadoop is not allowed to impersonate hadoop</a></li><li><a href="https://blog.csdn.net/mm_bit/article/details/52062488" target="_blank" rel="noopener">hbase错误：Org.apache.hadoop.ipc.RemoteException:User:client is not allowed to impersonate root</a></li></ol><h2 id="编程测试"><a href="#编程测试" class="headerlink" title="编程测试"></a>编程测试</h2><p>接下来就可以写代码运行了，在运行代码之前记得重启一下虚拟机（之前修改过配置文件，重启刷新一下配置），开机后重新开启所需的服务</p><pre><code class="python">from pyhive import hiveconn = hive.Connection(host=&#39;127.0.0.1&#39;, port=10000,  database=&#39;hive&#39; ,username=&#39;hadoop&#39;)cursor=conn.cursor()cursor.execute(&#39;select * from stu&#39;)for result in cursor.fetchall():     print(result)conn.close()</code></pre><p>这里面的stu这个表是我之前导入进去的，现在尝试读取一下这个表的内容</p><p><img src="http://120.24.1.93/blogimg/4/561_1.png" alt="561_1.png"></p><p><img src="http://120.24.1.93/blogimg/4/563_1.png" alt="563_1.png"></p><p>代码查询的结果与命令行查询的结果一致，没有什么问题。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>安装thrift并使用python3驱动Hbase</title>
      <link href="/2019/02/11/3/"/>
      <url>/2019/02/11/3/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>做比赛的过程中需要用到python去驱动hadoop的那一些东西，要安装thrift。因为按照版本不同等等原因，按照网上别人博客的教程配置并不能成功。经过自己处理问题之后将我自己的安装过程记录下来，作为自己的笔记以及让队伍其他成员使用。</p><p>这份博客可以有效解决网上比较常见的python3无法驱动hbase-thrift的问题（直接pip3 install hbase-thrift，就马上使用的话，在import的时候会爆出语法错误，因为python目录下site-packages/hbase/Hbase.py这个文件默认是用python2语法编写的，不兼容python3 ）。</p><p>网上也有人上传了修改过的Hbase.py文件，但是要么就是收钱，要么就是链接失效，甚至千辛万苦弄到手往往还不能成功跑起来。</p><p>我找到的解决方案就是自己编译安装thrift这个软件，然后用thrift在自己本机上重新生成一遍Hbase.py。这样做可以完美的在自己机子上用python3驱动Hbase。</p><p>如果有不是很熟悉hadoop的新人看到我这篇文章的话，请注意，我自己安装的thrift和Hbase安装文件夹下面自带的thrift服务是不一样的。前者是用来给客户端生成映射文件，后者是用来开启网络服务的。可以简单的认为一个工作在客户端，一个工作在服务端。如果只需要使用python2的话，不需要前者也可以正常驱动Hbase。因为pip下载下来的映射文件本身就是正确的。</p><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>这里使用的是ubuntu16.04虚拟机，崭新的系统，除了安装hadoop、Hbase、Hive、mysql以外并未额外安装其他软件</p><p>hadoop安装目录：/usr/local/hadoop</p><p>Hbase安装目录：/usr/local/hbase</p><p>Hbase源码包解压后的路径：/home/hadoop/Desktop/hbase-2.0.4</p><p>虚拟机用户名：hadoop</p><p>从官网download板块那里下载文件，我用的是0.12.0版本 <a href="http://thrift.apache.org" target="_blank" rel="noopener">apache thrift官网</a></p><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><p>首先按照官网的最新文档进行依赖的安装，网上那些教程安装的东西不太一样，在编译0.12.0版本thrift的时候会报错说缺东西</p><pre><code>sudo apt-get install automake bison flex g++ git libboost-all-dev libevent-dev libssl-dev libtool make pkg-config</code></pre><p>因为需要设置python支持（废话，不设置的话，怎么用python驱动），如果不安装这些，在编译安装的时候会提示找不到python.h</p><p>官网上说，如果要添加python支持，需要装上这些。</p><pre><code class="sh">sudo apt-get install python-all python-all-dev python-all-dbg</code></pre><p>实际上在我的ubuntu上面，光是执行上面这条并不够，还要再加一句。否则还是会爆红，说找不到python.h</p><pre><code class="sh">sudo apt-get install  python3.5-dev</code></pre><p>如果已经装过python2和python3对应的pip，那么上面这两步似乎可以省略，会提示这些软件已经存在了</p><p>安装完依赖之后主要检查一下这两个目录里面有没有 python.h这个文件（两个目录都要有）</p><pre><code class="sh">ls /usr/include/python2.7ls /usr/include/python3.5</code></pre><h2 id="编译安装软件"><a href="#编译安装软件" class="headerlink" title="编译安装软件"></a>编译安装软件</h2><p>接下来是正常的软件编译安装过程，贴这个代码主要是让你们看看configure参数</p><pre><code class="sh">tar -zxvf thrift-0.12.0.tar.gz cd thrift-0.12.0/./configure --prefix=/usr/local/thrift --with-cpp --with-boost --with-python --without-csharp --with-java --without-erlang --without-perl --with-php --without-php_extension --without-ruby --without-haskell --without-gomakesudo make install</code></pre><p>正常情况下应该不会爆红的，如果编译或者安装的时候爆红，自行想办法处理</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>安装pip，并使用pip安装两个模块，其中最容易出问题的是hbase-thrift</p><pre><code class="sh">sudo apt-get install python3-pippip3 install thriftpip3 install hbase-thrift</code></pre><p>解压hadoop的源码包<br>我是解压到了这个目录<br><img src="http://120.24.1.93/blogimg/3/547_0.png" alt="547_0.png"></p><pre><code class="sh">cd ~/Desktop/hbase-2.0.4/hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift/usr/local/thrift/bin/thrift -gen py Hbase.thrift</code></pre><p>会得到一个gen-py文件夹，里面有个文件夹叫hbase，用它去替换你pip3安装的那个hbase-thrift里面的hbase文件夹</p><pre><code class="sh">rm -r  /home/hadoop/.local/lib/python3.5/site-packages/hbasecp -r ~/Desktop/hbase-2.0.4/hbase-thrift/src/main/resources/org/apache/hadoop/hbase/thrift/gen-py/hbase /home/hadoop/.local/lib/python3.5/site-packages/</code></pre><p>如下图所示，尝试导入一下这个库，import成功就没问题了<br><img src="http://120.24.1.93/blogimg/3/549_0.png" alt="549_0.png"></p><p>尝试用python驱动一下看看，在这之前要先把hbase启动起来</p><p>启动hadoop</p><pre><code class="sh">start-all.sh</code></pre><p>启动Hbase</p><pre><code class="sh">start-hbase.sh</code></pre><p>启动Hbase的thrift服务</p><pre><code class="sh">hbase-daemon.sh start thrift</code></pre><p>上面这几个脚本我是添加过环境变量的，使用的时候不需要加路径，如果没添加环境变量，请自行根据安装目录补全路径来运行，如：</p><pre><code class="sh">/usr/local/hadoop/sbin/start-all.sh/usr/local/hbase/bin/start-hbase.sh/usr/local/hbase/bin/hbase-daemon.sh start thrift</code></pre><p>所有服务都启动完成之后，编写一段python代码，运行它</p><pre><code class="python">from thrift.transport import TSocket,TTransportfrom thrift.protocol import TBinaryProtocolfrom hbase import Hbase# thrift默认端口是9090socket = TSocket.TSocket(&#39;127.0.0.1&#39;,9090)socket.setTimeout(5000)transport = TTransport.TBufferedTransport(socket)protocol = TBinaryProtocol.TBinaryProtocol(transport)client = Hbase.Client(protocol)socket.open()print(client.getTableNames())  # 获取当前所有的表名</code></pre><p>这段demo代码是从别人的博客抄过来的，原文地址<br><a href="https://blog.csdn.net/luanpeng825485697/article/details/81048468" target="_blank" rel="noopener">https://blog.csdn.net/luanpeng825485697/article/details/81048468</a></p><p>运行结束之后程序输出了一个空的列表<br><img src="http://120.24.1.93/blogimg/3/551_0.png" alt="551_0.png"></p><p>进入hbase shell，用Hbase的语句创建一个表</p><pre><code class="sh">hbase shellhbase(main):002:0&gt; create &#39;student&#39;,&#39;Sname&#39;,&#39;Ssex&#39;,&#39;Sage&#39;,&#39;Sdept&#39;,&#39;course&#39;</code></pre><p>再运行一遍python代码可以观察得出，能成功新创建的表名了</p><p><img src="http://120.24.1.93/blogimg/3/553_0.png" alt="553_0.png"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hbase </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>对几个微信公众号2018年到现在所发表爬虫文章的简单整理</title>
      <link href="/2019/02/10/0002/"/>
      <url>/2019/02/10/0002/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>因为这阵子做某个小项目的原因，需要了解一下现在的爬虫玩家都喜欢爬哪些网站。就趁着无聊的时候打开微信，挑选了几个经常发爬虫文章的公众号，将它们从2018年1月至今的爬虫文章都收集了一下，并做个简单的分类，用来给我自己和需要的同学作为参考。</p><p>之所以不提供更多的数量，有三个原因：</p><ul><li>只需要大概做个参考，不需要太多数量</li><li>比较新的文章对我而言更有参考意义，有时候需要借鉴爬虫的代码，过旧的文章可能代码会失效</li><li>个人水平比较菜，一时半会儿折腾不出爬取公众号的爬虫</li></ul><h2 id="影视"><a href="#影视" class="headerlink" title="影视"></a>影视</h2><p><a href="http://url.cn/51H9glZ" target="_blank" rel="noopener"> 我爬取豆瓣影评，告诉你《复仇者联盟3》在讲什么？（内附源码） </a></p><p><a href="http://url.cn/5o31Qtp" target="_blank" rel="noopener">《邪不压正》到底怎么样？我爬取了上万条网友评论进行分析  </a></p><p><a href="http://url.cn/5deGHsk" target="_blank" rel="noopener"> 3天9亿！我爬取上万条评论解读《西虹市首富》并预测票房 </a></p><p><a href="http://url.cn/5fD2uNQ" target="_blank" rel="noopener"> 用Python解读分析《爱情公寓》电影版 </a></p><p><a href="https://url.cn/51kDwO6" target="_blank" rel="noopener"> 数十万网友点评：开心麻花再差，也强过张艺谋和李易峰？ </a></p><p><a href="http://url.cn/5aZ1IhA" target="_blank" rel="noopener"> 用Python分析豆瓣，评分最低的演员原来是他 </a></p><p><a href="http://url.cn/5KtTkD4" target="_blank" rel="noopener"> 多种方法爬取猫眼电影并分析（附代码） </a></p><p><a href="http://url.cn/5m2gNzx" target="_blank" rel="noopener">抖音小姐姐视频爬虫  </a></p><p><a href="http://url.cn/5vl7hbI" target="_blank" rel="noopener"> Python数据可视化：2018年电影分析 </a></p><p><a href="http://url.cn/5BVyxEt" target="_blank" rel="noopener"> 我分析了B站的弹幕，看到底啥是佩奇 </a></p><p><a href="http://url.cn/5Pe6wYH" target="_blank" rel="noopener"> 170行代码爬取《白蛇：缘起》短评数据 </a></p><p><a href="http://url.cn/53YgQ1e" target="_blank" rel="noopener"> 用Python解读最值得期待的春节档影片是…… </a></p><p><a href="http://url.cn/5W5Vwfw" target="_blank" rel="noopener"> 用Python全自动下载抖音视频！ </a></p><p><a href="http://url.cn/5XTfkps" target="_blank" rel="noopener"> 爬取了 48048 条评论数据，解读 9.3 分的《毒液》是否值得一看？ </a></p><p><a href="http://url.cn/53ah1DM" target="_blank" rel="noopener"> 11万部影视作品揭秘，掌握过半GRE单词，居然只需看懂20部电影？ </a></p><p><a href="http://url.cn/5RnOaQ1" target="_blank" rel="noopener">手把手教你用Python分析豆瓣电影——以《我不是药神》《邪不压正》为例  </a></p><p><a href="http://url.cn/51cHlLc" target="_blank" rel="noopener"> 使用 Python 分析《我不是药神》豆瓣电影短评 </a></p><p><a href="http://url.cn/5fiIXFn" target="_blank" rel="noopener">  用Python来看3天破10亿的《我不是药神》到底神在哪？   </a></p><p><a href="http://url.cn/5ejD71I" target="_blank" rel="noopener">  用Python写一个抖音漂亮小姐姐发掘器（附代码教程）   </a></p><p><a href="http://url.cn/5Aqha6Z" target="_blank" rel="noopener">   你还在付费爱奇艺VIP？神级程序员教你用Python任意下！  </a></p><p><a href="http://url.cn/569TZCA" target="_blank" rel="noopener">  Python 告诉你绝不知道的 1983-2018 春晚   </a></p><p><a href="http://url.cn/5Y7QLsb" target="_blank" rel="noopener">   史上最强春节档来袭！Python 解读哪部影片值得一看？  </a></p><p><a href="http://url.cn/5U6lTU9" target="_blank" rel="noopener">  春晚鬼畜 B 站日排行最高，赵本山：我的时代还没有结束！   </a></p><p><a href="http://url.cn/53mi3PL" target="_blank" rel="noopener">   上映 10 天，票房就突破 10 亿的《海王》真的有那么好看？  </a></p><p><a href="http://url.cn/5oR1z9Q" target="_blank" rel="noopener">   Python 爬取爱奇艺腾讯视频 250,000 条数据分析为什么李诞不值得了？  </a></p><p><a href="http://url.cn/5kx0aA8" target="_blank" rel="noopener">  Python 爬取爱奇艺 52432 条数据分析谁才是《奇葩说》的焦点人物？   </a></p><p><a href="http://url.cn/5Vski1N" target="_blank" rel="noopener"> Python 爬取猫眼数据分析《无名之辈》为何能逆袭成黑马？    </a></p><p><a href="http://url.cn/5MADDfs" target="_blank" rel="noopener">  Python 爬取 B 站数据分析，宋智孝李光洙谁最受中国粉丝喜爱  </a></p><p><a href="http://url.cn/5ko2jc8" target="_blank" rel="noopener">   纵观 30 年 5000 多部国产电视剧，豆瓣评分最低的演员原来是……  </a></p><p><a href="http://url.cn/53YVIbG" target="_blank" rel="noopener">  爬取了 B 站上的 17398 条评论，分析这部二次元番剧为何受到技术宅的追捧？   </a></p><p><a href="http://url.cn/5YKJK4K" target="_blank" rel="noopener">   爬取了《悲伤逆流成河》15195 条猫眼数据，满分好评背后靠的是什么？  </a></p><p><a href="http://url.cn/5e9EIzI" target="_blank" rel="noopener">  用 Python 爬取 500 条豆瓣影评，看看《蚁人2》是否有看点？   </a></p><p><a href="http://url.cn/5LUFRb6" target="_blank" rel="noopener">  分析日播放量超 6 亿的《延禧攻略》，看爱奇艺腾讯视频谁胜谁败   </a></p><p><a href="http://url.cn/5q7IXwu" target="_blank" rel="noopener">  Python 爬取猫眼千页评论，分析《狄仁杰之四大天王》是否值得一看   </a></p><p><a href="http://url.cn/5cbjlv2" target="_blank" rel="noopener"> 程序员用Python分析徐峥，竟研究出《我不是药神》30亿票房真正秘诀！    </a></p><p><a href="http://url.cn/5eyZO9c" target="_blank" rel="noopener">  Python 告诉你绝不知道的1983-2018 春晚   </a></p><p><a href="http://url.cn/5agYQBb" target="_blank" rel="noopener"> Python 分析《爱情公寓》电影，让我十年情怀一瞬间都喂了狗 </a></p><h2 id="音乐"><a href="#音乐" class="headerlink" title="音乐"></a>音乐</h2><p><a href="http://url.cn/5T8BJji" target="_blank" rel="noopener"> 用Python爬取陈奕迅新歌《我们》10万条评论的新发现 </a></p><p><a href="http://url.cn/52SQ10x" target="_blank" rel="noopener"> 爬取网易云音乐单曲下的所有评论 </a></p><p><a href="http://url.cn/5RuibFs" target="_blank" rel="noopener"> 用Python爬取中国新说唱歌曲信息 </a></p><p><a href="http://url.cn/5c2gFCr" target="_blank" rel="noopener">网易云音乐歌曲评论爬虫（附源码）  </a></p><p><a href="http://url.cn/5urzbiL" target="_blank" rel="noopener">  爬取了陈奕迅新歌《我们》10万条评论数据发现：原来，有些人只适合遇见   </a></p><p><a href="http://url.cn/5DsNR1P" target="_blank" rel="noopener">  爬取网易云音乐的评论后，竟有这种发现！   </a></p><p><a href="http://url.cn/5Rue0Ok" target="_blank" rel="noopener">  如何用 Python 爬取网易云音乐的 10w+ 评论？附详细代码解读   </a></p><p><a href="http://url.cn/5BGLFAa" target="_blank" rel="noopener">   爬取了陈奕迅新歌《我们》10万条评论数据发现：原来，有些人只适合遇见  </a></p><p><a href="http://url.cn/59rXpnU" target="_blank" rel="noopener">   Skr！如何用 Python 爬取中国新说唱热门歌曲？  </a></p><h2 id="文学"><a href="#文学" class="headerlink" title="文学"></a>文学</h2><p><a href="http://url.cn/5WtGEkE" target="_blank" rel="noopener"> Python自然语言处理分析倚天屠龙记 </a></p><p><a href="http://url.cn/5XEJXeJ" target="_blank" rel="noopener"> 用文本挖掘剖析近5万首《全唐诗》 </a></p><p><a href="http://url.cn/5JShKs7" target="_blank" rel="noopener"> 用Python分析《斗破苍穹》 </a></p><p><a href="http://url.cn/5b1FJoi" target="_blank" rel="noopener">  Python爬虫实战：爬取全站小说排行榜   </a></p><p><a href="http://url.cn/5OWU9AN" target="_blank" rel="noopener">   用 Python 分析《斗破苍穹》，分析其究竟是烂片无疑还是沧海遗珠？  </a></p><h2 id="房价"><a href="#房价" class="headerlink" title="房价"></a>房价</h2><p><a href="http://url.cn/5liCnl3" target="_blank" rel="noopener"> 用Python分析北京二手房房价 </a></p><p><a href="http://url.cn/5BSaf6Z" target="_blank" rel="noopener">   爬取了 31502 条北京自如租房数据，看看是否居者有其屋？  </a></p><p><a href="http://url.cn/5KReaPs" target="_blank" rel="noopener">   Python 爬取了 1.7 万条房产数据，告诉你深圳的生存压力究竟有多大！  </a></p><p><a href="http://url.cn/5Kj9y9x" target="_blank" rel="noopener">   爬取两万多条租房数据，算算在广州你能「活到」第几集？  </a></p><p><a href="http://url.cn/56hg1rF" target="_blank" rel="noopener"> 北京房租大涨？6个维度，数万条数据帮你揭穿 </a></p><h2 id="职位"><a href="#职位" class="headerlink" title="职位"></a>职位</h2><p><a href="http://url.cn/5VNWHMv" target="_blank" rel="noopener"> Python拉勾网数据采集与可视化 </a></p><p><a href="http://url.cn/5hOR1Wn" target="_blank" rel="noopener">用爬虫分析互联网大数据行业薪资情况  </a></p><p><a href="http://url.cn/50p4yGP" target="_blank" rel="noopener">用Python给互联网公司打分，你在的公司几分？  </a></p><p><a href="http://url.cn/5NPzZug" target="_blank" rel="noopener"> 10行代码爬取全国所有A股/港股/新三板上市公司信息 </a></p><p><a href="http://url.cn/5lxWC5c" target="_blank" rel="noopener">  Python 爬取 4027 条脉脉职言，解读互联网人的苦与难！   </a></p><p><a href="http://url.cn/5Q0Qoh0" target="_blank" rel="noopener">  Python爬取历年招聘数据，告诉你如何成为备受追捧的数据分析工程师！   </a></p><p><a href="http://url.cn/5LzIlaw" target="_blank" rel="noopener">   用 Python 爬了 10 万条软件测试工程师信息有这些发现！  </a></p><p><a href="http://url.cn/5rIj5Vj" target="_blank" rel="noopener">  用 Python 爬取 7 万运营人画像有这些重大发现！  </a></p><p><a href="https://mp.weixin.qq.com/s/b_JL4Cb3-kdzUDijxp7RYw" target="_blank" rel="noopener">  分析了 160 多万个招聘职位，竟发现……   </a></p><h2 id="社交平台"><a href="#社交平台" class="headerlink" title="社交平台"></a>社交平台</h2><p><a href="http://url.cn/5R9i5bV" target="_blank" rel="noopener"> 用Python爬取知乎上关于程序员的神回复 </a></p><p><a href="http://url.cn/5GnfXYK" target="_blank" rel="noopener"> Python抓取分析“创造101 ”菊姐微博 </a></p><p><a href="http://url.cn/5b4s7di" target="_blank" rel="noopener"> Python 3.6实现单博主微博文本、图片及热评爬取 </a></p><p><a href="http://url.cn/5a0xjRc" target="_blank" rel="noopener"> 用Python对用户评论典型意见进行数据挖掘 </a></p><p><a href="http://url.cn/5DfULQE" target="_blank" rel="noopener"> 60行Python代码爬取知乎神回复 </a></p><p><a href="http://url.cn/5uyPyPn" target="_blank" rel="noopener">  一小时爬千万数据的新浪微博爬虫   </a></p><p><a href="http://url.cn/5GnRhhe" target="_blank" rel="noopener"> 微信公众号爬虫weixin_crawler开源啦 </a></p><p><a href="http://url.cn/52j3380" target="_blank" rel="noopener">  基于Python实现微信公众号爬虫进行数据分析   </a></p><p><a href="http://url.cn/5zYTBZc" target="_blank" rel="noopener">   Python 爬取 6000 篇文章分析 CSDN 是如何进入微信 500 强的  </a></p><p><a href="http://url.cn/5IqYTAA" target="_blank" rel="noopener">   我扒了 6730 个微信用户数据，得出了这些结论……  </a></p><p><a href="http://url.cn/5LutagB" target="_blank" rel="noopener">  如何用 Python 测出你和微信好友的亲密程度？   </a></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p><a href="http://url.cn/5d7lx2O" target="_blank" rel="noopener"> 用Python爬取分析全国旅游数据 </a></p><p><a href="http://url.cn/5Ppqy3c" target="_blank" rel="noopener"> Python数据可视化：2018年北上广深空气质量分析（附完整代码） </a></p><p><a href="http://url.cn/5cuVvxQ" target="_blank" rel="noopener"> 金州勇士4年3冠的成功秘诀！Python数据可视化分析告诉你答案 </a></p><p><a href="http://url.cn/53GsZu7" target="_blank" rel="noopener"> 我爬取分析美团网，原来北京上海Top10美食是它们 </a></p><p><a href="http://url.cn/5hhiuRW" target="_blank" rel="noopener"> 利用robots.txt快速抓取网站的小窍门 </a></p><p><a href="http://url.cn/59fQPNo" target="_blank" rel="noopener"> Python 分析新垣结衣高圆圆佟丽娅，程序员最想娶的女星竟然是… </a></p><p><a href="http://url.cn/5Ba6YQF" target="_blank" rel="noopener"> 点评网的反爬再也不是烦恼 </a></p><p><a href="http://url.cn/5wOiE1D" target="_blank" rel="noopener"> 写文章不会起标题？爬取虎嗅5万篇文章告诉你 </a></p><p><a href="http://url.cn/5OlRMID" target="_blank" rel="noopener"> Python数据分析实战 | 月饼之王花落谁手？ </a></p><p><a href="http://url.cn/5ZRxO9L" target="_blank" rel="noopener"> 用Python分析了20万场吃鸡数据 </a></p><p><a href="http://url.cn/5QJS3AC" target="_blank" rel="noopener"> 我用Python爬取了14年所有的福彩3D信息，彩民们，只能帮你们到这了 </a></p><p><a href="http://url.cn/5ahxvXK" target="_blank" rel="noopener">  用 Python 分析4W场球赛，2018 世界杯冠军竟是……   </a></p><p><a href="http://url.cn/55zdgmA" target="_blank" rel="noopener">   用Python来统计知识星球打卡作业  </a></p><p><a href="http://url.cn/5sDS5zo" target="_blank" rel="noopener">  程序媛爬取了 5 个 APP 的 4220 个数据，解读过去的一年到底过得怎么样！   </a></p><p><a href="http://url.cn/5o0LnvK" target="_blank" rel="noopener">   如何基于谷歌的最强 NLP 模型进行影评分析？  </a></p><p><a href="http://url.cn/56jPl3u" target="_blank" rel="noopener">   Python 爬下的必胜客数据背后，藏着什么样的信息？  </a></p><p><a href="http://url.cn/5h4y8AA" target="_blank" rel="noopener">  Python 分析 35 年的考研英语真题词汇，解读孤独的考研大军！   </a></p><p><a href="http://url.cn/5xcmExF" target="_blank" rel="noopener">  用 Python 分析了 1982 场英雄联盟数据，开局前预测游戏对局胜负！   </a></p><p><a href="http://url.cn/5dqwtFf" target="_blank" rel="noopener">   用 Python 分析了 10000 场吃鸡数据，原来吃鸡要这么玩！  </a></p><p><a href="http://url.cn/5yTu8N5" target="_blank" rel="noopener"> 如何用 Python 一键搞定 CET 4 最爱考的 5000 个单词？   </a></p><p><a href="http://url.cn/5GiFDKu" target="_blank" rel="noopener">   分析 120 年的 Kaggle 数据，中国何时站上奥运之巅？  </a></p><p><a href="http://url.cn/5OcJnXZ" target="_blank" rel="noopener">   程序员再花 28 小时分析各省疫苗采购公示数据，竟有这些发现！  </a></p><p><a href="http://url.cn/51onZn2" target="_blank" rel="noopener">   用 Python 爬取了 14 年的福彩 3D 信息！彩民们，只能帮你们到这了  </a></p><p><a href="http://url.cn/5Gy08dG" target="_blank" rel="noopener">  Python 爬取 50 万数据得出最权威、全面的高考志愿填报指南！   </a></p><p><a href="http://url.cn/5b7KqNK" target="_blank" rel="noopener">  如何用 100 行 Python 代码实现新闻爬虫？   </a></p><p><a href="http://url.cn/5b52CgX" target="_blank" rel="noopener"> 一文教你如何用Python预测股票价格    </a></p><p><a href="http://url.cn/5bk21Pl" target="_blank" rel="noopener">  用python分析《三国演义》中的社交网络   </a></p><p><a href="http://url.cn/5hjJmXm" target="_blank" rel="noopener">   Python 爬取大众点评 50 页数据，最好吃的成都火锅竟是它！  </a></p><p><a href="http://url.cn/5B5MxYI" target="_blank" rel="noopener">  Python爬虫&amp;可视化-舌尖上的“小龙虾”  </a></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>本博客今天起投入使用</title>
      <link href="/2018/12/04/0001/"/>
      <url>/2018/12/04/0001/</url>
      
        <content type="html"><![CDATA[<p>在未来的一段时间内，我将会把学习和项目中遇到的技术问题及其解决方案撰写成文发到这里，与各位程序员共同分享，也作为我自己的一个笔记。</p>]]></content>
      
      
      <categories>
          
          <category> test </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 标签1 </tag>
            
        </tags>
      
    </entry>
    
  
  
    
    <entry>
      <title>推荐链接</title>
      <link href="/links/index.html"/>
      <url>/links/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    <entry>
      <title>时间轴</title>
      <link href="/timeline/index.html"/>
      <url>/timeline/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    <entry>
      <title>标签云</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
  
</search>
